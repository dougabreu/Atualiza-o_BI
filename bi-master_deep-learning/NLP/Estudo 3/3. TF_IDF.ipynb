{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEnS/VAmrQL13BjqBdhyA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crismunoz/NLP_examples/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceIpd_flItH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "dcec3b58-c83c-4abb-f6d7-8cb4a2fc3d05"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "import bs4 as bs  \n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwWPG5OYKpqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_html = urllib.request.urlopen('https://blogs.oglobo.globo.com/miriam-leitao/post/azevedo-versus-trump-o-contexto.html')  \n",
        "raw_html = raw_html.read()\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPd21mgnLdI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_paragraphs = article_html.article.find_all('p')\n",
        "article_text = ''\n",
        "for para in article_paragraphs:  \n",
        "    article_text += para.text"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z8wJ45TLiqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = nltk.sent_tokenize(article_text)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1N_VVVKNIGK",
        "colab_type": "text"
      },
      "source": [
        "# Counter Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA950sVDNELM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "715d42e9-cb6d-435e-b4c2-82bfa208ee36"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcA0AP2zNjT2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "f10c973e-bea5-4234-ef95-89b333ee59b4"
      },
      "source": [
        "# use default nltk tokenizer\n",
        "vectorizer.set_params(tokenizer=nltk.word_tokenize)\n",
        "\n",
        "# remove Portuguese stop words\n",
        "vectorizer.set_params(stop_words=stopwords)\n",
        "\n",
        "# include 1-grams and 2-grams\n",
        "vectorizer.set_params(ngram_range=(1, 2))\n",
        "\n",
        "# ignore terms that appear in more than 50% of the documents\n",
        "vectorizer.set_params(max_df=0.5)\n",
        "\n",
        "# only keep terms that appear in at least 2 documents\n",
        "vectorizer.set_params(min_df=2)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=0.5, max_features=None, min_df=2,\n",
              "                ngram_range=(1, 2), preprocessor=None,\n",
              "                stop_words=['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em',\n",
              "                            'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se',\n",
              "                            'na', 'por', 'mais', 'as', 'dos', 'como', 'mas',\n",
              "                            'ao', 'ele', 'das', 'à', 'seu', 'sua', ...],\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<function word_tokenize at 0x7f624562e9d8>,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hFFCeUcNL2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "4c5a64b1-0214-4782-c7a3-fc68196b65e1"
      },
      "source": [
        "X = vectorizer.fit_transform(corpus)\n",
        "X.toarray()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 1],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrwWzoV2NdaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "71bc74c2-5d46-4c41-d393-e8c440c64d53"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['?', 'ameaça', 'americano', 'ano', 'anos', 'azevêdo', 'azevêdo ,', 'azevêdo deixou', 'biden', 'bob', 'brasileiro', 'conta', 'contexto', 'conversa', 'deixou', 'desde', 'diretor-geral', 'disse', 'disse azevêdo', 'diz', 'dois', 'encontro', 'encontro ,', 'eua', 'excelente', 'excelente .', 'fato', 'governo', 'importante', 'janeiro', 'mentirosa', 'mentiroso', 'nunca', 'omc', 'omc ,', 'organização', 'outro', 'presidente', 'presidente americano', 'sendo', 'testemunhas', 'trump', 'trump .', 'woodward', '“', '”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORus2oZiODL9",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCHf-H1VMFeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "95337d26-397a-4366-f47d-cad26f0a619c"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgDncu7aMDwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "763717cf-6df1-4ebf-ccd3-87b724d9ad92"
      },
      "source": [
        "# use default nltk tokenizer\n",
        "vectorizer.set_params(tokenizer=nltk.word_tokenize)\n",
        "\n",
        "# remove Portuguese stop words\n",
        "vectorizer.set_params(stop_words=stopwords)\n",
        "\n",
        "# include 1-grams and 2-grams\n",
        "vectorizer.set_params(ngram_range=(1, 2))\n",
        "\n",
        "# ignore terms that appear in more than 50% of the documents\n",
        "vectorizer.set_params(max_df=0.5)\n",
        "\n",
        "# only keep terms that appear in at least 2 documents\n",
        "vectorizer.set_params(min_df=2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
              "                min_df=2, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True,\n",
              "                stop_words=['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em',\n",
              "                            'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se',\n",
              "                            'na', 'por', 'mais', 'as', 'dos', 'como', 'mas',\n",
              "                            'ao', 'ele', 'das', 'à', 'seu', 'sua', ...],\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<function word_tokenize at 0x7f624562e9d8>,\n",
              "                use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pev1JzNL34C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "54bf093d-2016-4456-c919-a954ea2e85e6"
      },
      "source": [
        "X = vectorizer.fit_transform(corpus)\n",
        "X.toarray()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.1923588 , 0.        , 0.        , ..., 0.1923588 , 0.17168473,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.31332825, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUYHXOX0L-zq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fa238243-6bca-4343-c507-3a9cf4b33f3c"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['?', 'ameaça', 'americano', 'ano', 'anos', 'azevêdo', 'azevêdo ,', 'azevêdo deixou', 'biden', 'bob', 'brasileiro', 'conta', 'contexto', 'conversa', 'deixou', 'desde', 'diretor-geral', 'disse', 'disse azevêdo', 'diz', 'dois', 'encontro', 'encontro ,', 'eua', 'excelente', 'excelente .', 'fato', 'governo', 'importante', 'janeiro', 'mentirosa', 'mentiroso', 'nunca', 'omc', 'omc ,', 'organização', 'outro', 'presidente', 'presidente americano', 'sendo', 'testemunhas', 'trump', 'trump .', 'woodward', '“', '”']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
