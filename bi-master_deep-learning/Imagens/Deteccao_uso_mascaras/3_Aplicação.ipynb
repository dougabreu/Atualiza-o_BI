{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Aplicação.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKD7B2L2ML+o7bmB/QP7eE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b0c08c96278d4efab81641be0354faf1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d78374a19bce462697637e987ed85fdf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1cb20774f92a4426b3aad64067d8bf18","IPY_MODEL_c1b5774d719d4301af87a9e991f5de28"]}},"d78374a19bce462697637e987ed85fdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1cb20774f92a4426b3aad64067d8bf18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_488d6a5b24df4382aba8962b2a2ca15d","_dom_classes":[],"description":"Processing video:: ","_model_name":"FloatProgressModel","bar_style":"success","max":1389,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1389,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_494ca3385bae4fdc8e2ee8d439133e62"}},"c1b5774d719d4301af87a9e991f5de28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_753a763653ed45198638bd1bbe0f70a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1390/? [07:18&lt;00:00,  3.17it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21e59719f9774dddaac8e15408460c3c"}},"488d6a5b24df4382aba8962b2a2ca15d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"494ca3385bae4fdc8e2ee8d439133e62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"753a763653ed45198638bd1bbe0f70a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"21e59719f9774dddaac8e15408460c3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"OgCB4knONRNw","colab_type":"text"},"source":["# Aplicação: Detecção de uso de máscaras\n","Os passos para aplicar nosso classificador são os seguintes:\n","\n","1.   Carregar modelo treinado\n","2.   Obter cada frame do video\n","2.   Detecção de rostos em cada frame\n","3.   Extração de cada rosto\n","4.   Aplicar o classificador\n","5.   Mostrar a classe prevista no frame\n","6.   Salvar frame no video\n","\n","Verificar que o tipo de ambiente de execução é **GPU**.\n","Para mudar o tipo de ambiente de execução, ir para:\n","\n","`Ambiente de execução -> Alterar tipo de ambiente de execução`\n","\n","Em Acelerador de hardware, selecionar **GPU**"]},{"cell_type":"markdown","metadata":{"id":"RdU3YxMNOIpt","colab_type":"text"},"source":["## Montar o Drive"]},{"cell_type":"markdown","metadata":{"id":"up8rXYIZOL_F","colab_type":"text"},"source":["Executando as seguinte linhas de codigo, podemos montar nosso Google Drive no Colab:\n","\n","```python\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","Depois, um link sera apresentado onde temos que accesar para obter um token. Finalmente temos que colar o token nosso Colab para permitir o acceso ao nossos arquivos."]},{"cell_type":"code","metadata":{"id":"PaOh6fBEEZlN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593689620397,"user_tz":180,"elapsed":762,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}},"outputId":"acf9e483-3e39-46f9-a963-13e1e8df5b98"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vaG6PzYEEl2g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593689623460,"user_tz":180,"elapsed":777,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}},"outputId":"361ee63a-9dba-4186-d1fc-6df618d347d4"},"source":["cd drive/My\\ Drive/Colabs/Mask_Detection"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colabs/Mask_Detection\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W42F7zGtOQvd","colab_type":"text"},"source":["## Instalar Dependencias"]},{"cell_type":"markdown","metadata":{"id":"fChQKWnrOR_l","colab_type":"text"},"source":["Neste caso, só vamos precisar da biblioteca [face-recognition](https://pypi.org/project/face-recognition/) para fazer a detecção de rostos e extrair os marcos faciais.\n","\n","\n","```\n","!pip install -U face-recognition\n","```\n"]},{"cell_type":"code","metadata":{"id":"aFRzO1EIJtGK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1593689629718,"user_tz":180,"elapsed":3355,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}},"outputId":"0a908ddf-2beb-4220-9feb-9765e3f5b323"},"source":["!pip install -U face-recognition"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: face-recognition in /usr/local/lib/python3.6/dist-packages (1.3.0)\n","Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from face-recognition) (7.0.0)\n","Requirement already satisfied, skipping upgrade: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face-recognition) (19.18.0)\n","Requirement already satisfied, skipping upgrade: face-recognition-models>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face-recognition) (0.3.0)\n","Requirement already satisfied, skipping upgrade: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face-recognition) (7.1.2)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from face-recognition) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ENi1FujYOZ99","colab_type":"text"},"source":["## Import das bibliotecas necessarias"]},{"cell_type":"code","metadata":{"id":"3eYGr7syLyv5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593689634731,"user_tz":180,"elapsed":3468,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}}},"source":["import numpy as np\n","import cv2\n","import face_recognition\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from tqdm import tqdm_notebook"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvCqmeFgOek1","colab_type":"text"},"source":["## Código Principal"]},{"cell_type":"markdown","metadata":{"id":"r6fWpkiQO2At","colab_type":"text"},"source":["Definimos os parâmetros fixos, neste caso são o nome do video a ser processado,\n","```python\n","input_video_filename = \"test_video.mp4\"\n","```\n","o nome do video de saida,\n","```python\n","output_video_filename = \"test_video_output.mp4\"\n","```\n","o tamanho dos frames,\n","```python\n","frame_size = (640,480)\n","```\n","e o modelo treinado, o qual carregamos com a função `load_model`\n","```python\n","model = load_model(\"model.h5\")\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"vIw4QKdyOmSl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593689639490,"user_tz":180,"elapsed":3719,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}}},"source":["input_video_filename = \"test_video.mp4\"\n","output_video_filename = \"test_video_output.mp4\"\n","frame_size = (640,480)\n","model = load_model(\"model.h5\")"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6ULrIhRPhjO","colab_type":"text"},"source":["Agora criamos o objeto para escrever o video processado. Temos que definir o tipo de codec a ser usado, os `frames-per-second` e o tamanho do frame:\n","```python\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","fps=20.0\n","out = cv2.VideoWriter(output_video_filename,fourcc, fps, frame_size)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"6DWmxOgaOoUG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593689640889,"user_tz":180,"elapsed":757,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}}},"source":["# Define the codec and create VideoWriter object\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","fps=20.0\n","out = cv2.VideoWriter(output_video_filename,fourcc, fps, frame_size)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPi3rPLBP4yT","colab_type":"text"},"source":["Inicializamos o objeto para a leitura de cada frame\n","\n","```python\n","cap = cv2.VideoCapture(input_video_filename)\n","```\n","e obtemos a quantidade total de frames no video\n","\n","\n","```python\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))1\n","```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"oVeFLqPbOugk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593689643331,"user_tz":180,"elapsed":750,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}}},"source":["cap = cv2.VideoCapture(input_video_filename)\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtAXcSEPQZUz","colab_type":"text"},"source":["### Processando cada frame"]},{"cell_type":"markdown","metadata":{"id":"aFh8guEYQNgc","colab_type":"text"},"source":["Começamos com um loop para obter cada frame do video:\n","```python\n","while (cap.isOpened()):\n","    ret, frame = cap.read()\n","```\n","Se o frame é valido, fazemos a conversão para RGB, mudamos o tamanho do frame e obtemos a localização de cada rosto na imagem:\n","```python    \n","    if ret:\n","      frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)      \n","      frame = cv2.resize(frame, frame_size)\n","      face_locations = face_recognition.face_locations(frame, model=\"cnn\")\n","```\n","Agora só se tiver pelo menos um rosto na imagem, iteramos sobre os rostos e extraimos as coordenadas do rosto:\n","```python \n","      if len(face_locations) is not 0:\n","        for face_location in face_locations:\n","          top, right, bottom, left = face_location\n","```\n","Extraimos o rosto da imagem e fazemos um resize para (224,224) que é o tamanho das imagens com as quais o modelo foi treinado:\n","```python\n","          face_image = frame[top:bottom, left:right]\n","          face_image = cv2.resize(face_image,(224,224))\n","```\n","Pre-processamos o rosto e acrescentamos uma dimensão para fazer a previsão usando o modelo treinado:\n","```python\n","          face_image = preprocess_input(face_image)\n","          face_image = np.expand_dims(face_image, axis=0)\n","\n","          (mask, withoutMask) = model.predict(face_image)[0]\n","```\n","O resultado são as probabilidades de ter ou não ter máscara. Definimos o cor verde `(0,255,0)` para os que tem máscara, e vermelho `(255,0,0)` para os casos que não tem máscara\n","```python\n","          label = \"Mask\" if mask > withoutMask else \"No Mask\"\n","          color = (0, 255, 0) if label == \"Mask\" else (255, 0, 0)\n","```\n","Mostramos um texto contendo a etiqueta prevista e a probabilidade. Também desenhamos um retângulo na localização do rosto\n","```python\n","          # include the probability in the label\n","          label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","          # display the label and bounding box rectangle on the output frame\n","          cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","          cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n","```\n","Finalmente, escrevemos cada frame e se não tiver mais frames, saímos do loop principal e liberamos os objetos para leitura e escrita do video.\n","```python\n","      out.write(frame)\n","    else:\n","      break\n","\n","out.release()\n","cap.release()\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"1jF-tMQwGHCq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["b0c08c96278d4efab81641be0354faf1","d78374a19bce462697637e987ed85fdf","1cb20774f92a4426b3aad64067d8bf18","c1b5774d719d4301af87a9e991f5de28","488d6a5b24df4382aba8962b2a2ca15d","494ca3385bae4fdc8e2ee8d439133e62","753a763653ed45198638bd1bbe0f70a3","21e59719f9774dddaac8e15408460c3c"]},"executionInfo":{"status":"ok","timestamp":1593690086699,"user_tz":180,"elapsed":439309,"user":{"displayName":"Pedro Marco Achanccaray Diaz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggy2vqWz_JVg7A7gONXwmbCAm89czFsr_T71SQrjQ=s64","userId":"00866306694178948894"}},"outputId":"30caf99c-52d1-44fa-8180-30af78162221"},"source":["pbar = tqdm_notebook(total=total_frames, desc=\"Processing video:\")\n","\n","while (cap.isOpened()):\n","    ret, frame = cap.read()\n","    pbar.update(1)\n","\n","    if ret:\n","      frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)      \n","      frame = cv2.resize(frame, frame_size)\n","      face_locations = face_recognition.face_locations(frame, model=\"cnn\")\n","      if len(face_locations) is not 0:\n","        for face_location in face_locations:\n","          top, right, bottom, left = face_location\n","          face_image = frame[top:bottom, left:right]\n","          face_image = cv2.resize(face_image,(224,224))        \n","          face_image = preprocess_input(face_image)\n","          face_image = np.expand_dims(face_image, axis=0)\n","\n","          (mask, withoutMask) = model.predict(face_image)[0]\n","\n","          label = \"Mask\" if mask > withoutMask else \"No Mask\"\n","          color = (0, 255, 0) if label == \"Mask\" else (255, 0, 0)\n","          # include the probability in the label\n","          label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","          # display the label and bounding box rectangle on the output frame\n","          cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","          cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n","      frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)      \n","      out.write(frame)\n","    else:\n","      break\n","\n","pbar.close()\n","out.release()\n","cap.release()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0c08c96278d4efab81641be0354faf1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Processing video:', max=1389.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]}]}